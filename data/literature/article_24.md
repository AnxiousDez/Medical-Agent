# Development and validation of a multimodal automatic interictal epileptiform discharge detection model: a prospective multi-center study.

<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Med</journal-id><journal-id journal-id-type="iso-abbrev">BMC Med</journal-id><journal-title-group><journal-title>BMC Medicine</journal-title></journal-title-group><issn pub-type="epub">1741-7015</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">12357389</article-id><article-id pub-id-type="pmid">40817241</article-id>
<article-id pub-id-type="publisher-id">4316</article-id><article-id pub-id-type="doi">10.1186/s12916-025-04316-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Development and validation of a multimodal automatic interictal epileptiform discharge detection model: a prospective multi-center study</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lin</surname><given-names>Nan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Lian</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Gao</surname><given-names>Weifang</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hu</surname><given-names>Peng</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Yuan</surname><given-names>Gonglin</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Heyang</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Qi</surname><given-names>Fang</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Lin</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Shengsong</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name><surname>Liang</surname><given-names>Zi</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>He</surname><given-names>Haibo</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Dong</surname><given-names>Yisu</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Gao</surname><given-names>Zaifen</given-names></name><address><email>gaozaifen@163.com</email></address><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Shao</surname><given-names>Xiaoqiu</given-names></name><address><email>shaoxiaoqiu2000@aliyun.com</email></address><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Cui</surname><given-names>Liying</given-names></name><address><email>pumchcuily@sina.com</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Lu</surname><given-names>Qiang</given-names></name><address><email>luqiang@pumch.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04jztag35</institution-id><institution-id institution-id-type="GRID">grid.413106.1</institution-id><institution-id institution-id-type="ISNI">0000 0000 9889 6335</institution-id><institution>Department of Neurology, </institution><institution>Peking Union Medical College Hospital, </institution></institution-wrap>NO.1 Shuaifuyuan Hutong of Dongcheng District, Beijing, 100730 China </aff><aff id="Aff2"><label>2</label>NetEase Media Technology Co., Ltd. Building No.7, West Zone, Zhongguancun Software Park (Phase II), No. 10 Xibeiwang East Road, Haidian District, Beijing, 100084 China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/023hj5876</institution-id><institution-id institution-id-type="GRID">grid.30055.33</institution-id><institution-id institution-id-type="ISNI">0000 0000 9247 7930</institution-id><institution>School of Computer Science and Technology, </institution><institution>Dalian University of Technology, </institution></institution-wrap>No.2 Linggong Road, Ganjingzi District, Dalian, 116024 China </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0207yh398</institution-id><institution-id institution-id-type="GRID">grid.27255.37</institution-id><institution-id institution-id-type="ISNI">0000 0004 1761 1174</institution-id><institution>Department of Neurology, </institution><institution>Children&#x02019;s Hospital Affiliated to Shandong University, </institution></institution-wrap>23976 Jing-Shi Road, Jinan, Shandong 250022 China </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/003regz62</institution-id><institution-id institution-id-type="GRID">grid.411617.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 0642 1244</institution-id><institution>Department of Neurology, </institution><institution>Beijing Tiantan Hospital, </institution></institution-wrap>No.119 South Fourth Ring West Road, Fengtai District, Beijing, 100070 China </aff><aff id="Aff6"><label>6</label>Department of Neurology, Tog Toh County Hospital, No. 95 Xinjian Middle Road, Shuanghe Town, Togtoh County, Hohhot, 010299 China </aff></contrib-group><pub-date pub-type="epub"><day>15</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>15</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>23</volume><elocation-id>479</elocation-id><history><date date-type="received"><day>18</day><month>4</month><year>2025</year></date><date date-type="accepted"><day>1</day><month>8</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p id="Par1">Visual identification of interictal epileptiform discharge (IED) is expert-biased and time-consuming. Accurate automated IED detection models can facilitate epilepsy diagnosis. This study aims to develop a multimodal IED detection model (vEpiNetV2) and conduct a multi-center validation.</p></sec><sec><title>Methods</title><p id="Par2">We constructed a large training dataset to train vEpiNetV2, which comprises 26,706 IEDs and 194,797 non-IED 4-s video-EEG epochs from 530 patients at Peking Union Medical College Hospital (PUMCH). The automated IED detection model was constructed using deep learning based on video and electroencephalogram (EEG) features. We proposed a bad channel removal model and patient detection method to improve the robustness of vEpiNetV2 for multi-center validation. Performance is verified in a prospective multi-center test dataset, with area under the precision-recall curve (AUPRC) and area under the curve (AUC) as metrics.</p></sec><sec><title>Results</title><p id="Par3">To fairly evaluate the model performance, we constructed a large test dataset containing 149 patients, 377&#x000a0;h video-EEG data, and 9232 IEDs from PUMCH, Children&#x02019;s Hospital Affiliated to Shandong University (SDQLCH) and Beijing Tiantan Hospital (BJTTH). Amplitude discrepancies are observed across centers and could be classified by a classifier. vEpiNetV2 demonstrated favorable accuracy for the IED detection, achieving AUPRC/AUC values of 0.76/0.98 (PUMCH), 0.78/0.96 (SDQLCH), and 0.76/0.98 (BJTTH), with false positive rates of 0.16&#x02013;0.31 per minute at 80% sensitivity. Incorporating video features improves precision by 9%, 7%, and 5% at three centers, respectively. At 95% sensitivity, video features eliminated 24% false positives in the whole test dataset. While bad channels decreased model precision, video features compensate for this deficiency. Accurate patient detection is essential; otherwise, incorrect patient detection can negatively impact overall performance.</p></sec><sec><title>Conclusions</title><p id="Par4">The multimodal IED detection model, which integrates video and EEG features, demonstrated high precision and robustness. The large multi-center validation confirmed its potential for real-world clinical application and the value of video features in IED analysis.</p></sec><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1186/s12916-025-04316-3.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Interictal epileptiform discharge detection</kwd><kwd>Deep learning</kwd><kwd>Multi-center validation</kwd><kwd>Multimodal</kwd><kwd>EEG</kwd><kwd>Video</kwd></kwd-group><funding-group><award-group><funding-source><institution>National Key Research and Development Project</institution></funding-source><award-id>2022YFC2503800</award-id><award-id>2022YFC2503800</award-id><award-id>2022YFC2503800</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution>CAMS Innovation Fund for Medical Sciences (CIFMS)</institution></funding-source><award-id>2023-I2M-C&#x00026;T-B-023</award-id><award-id>2023-I2M-C&#x00026;T-B-023</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution>Big Data Science and Technology Project of Jinan Municipal Health Commission</institution></funding-source><award-id>2022-YBD-17</award-id></award-group></funding-group><funding-group><award-group><funding-source><institution>Science and Technology Program of the Joint Fund of Scientific Research for the Public Hospitals of Inner Mongolia Academy of Medical Sciences</institution></funding-source><award-id>2024GLLH0452</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; BioMed Central Ltd., part of Springer Nature 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p id="Par28">Epilepsy is a common disease of the brain characterized by repeated seizures [<xref ref-type="bibr" rid="CR1">1</xref>], affecting approximately 50 million people worldwide [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. Interictal epileptiform discharges (IEDs) in an electroencephalogram (EEG) are typically observed in patients with epilepsy and manifest as spikes or sharp waves that stand out from background activities [<xref ref-type="bibr" rid="CR4">4</xref>]. While visual analysis by trained specialists remains the gold standard for IED detection [<xref ref-type="bibr" rid="CR5">5</xref>], this approach is labor-intensive, time-consuming, and subject to inter-expert variability. Meanwhile, technological advancements and improvements in patient care have led to an increase in the quantity and duration of EEG recordings globally [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR6">6</xref>]. This growing volume of data provides an opportunity to develop automated methods for efficient and accurate epileptiform discharges detection [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>].
</p><p id="Par29">Since the 1970s, automated EEG-based epileptiform activity detection has advanced significantly [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>]. There is notable progress in the application of artificial intelligence for IED detection [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>]. Among these, deep learning-based methods, particularly convolutional neural networks (CNNs), have emerged as powerful tools for EEG analysis. By leveraging large-scale datasets for training, these methods have demonstrated remarkable performance, offering a promising alternative to traditional approaches [<xref ref-type="bibr" rid="CR13">13</xref>].
</p><p id="Par30">In recent years, numerous deep learning algorithms have achieved high sensitivity when relying solely on EEG data [<xref ref-type="bibr" rid="CR14">14</xref>&#x02013;<xref ref-type="bibr" rid="CR22">22</xref>]. Deep neural network-based algorithms have been demonstrated to perform comparably to, or even surpass, EEG experts in detecting IEDs [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. Most studies have assessed detection model performance using cross-validation, reporting high specificity and sensitivity [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR23">23</xref>]. However, when validated on independent EEG datasets, separate from the training dataset, specificity often declined, particularly at high sensitivity levels [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. This discrepancy suggests that while deep learning models can achieve promising results in specific validation settings, their performance may not generalize well to real-world clinical applications, where EEG data exhibit significant variability.</p><p id="Par31">A multimodal detection model incorporating EEG and video data offers a potential solution to enhance the robustness of IED detection. EEG signals typically exhibit low amplitude and are susceptible to artifacts and noise from both patients and environmental conditions. These artifacts can interfere with IED detection, resulting in false positive outcomes. Video recordings provide visual information about the patient&#x02019;s conditions and movements, helping to discriminate IEDs from artifacts. In our previous multimodal method [<xref ref-type="bibr" rid="CR25">25</xref>], we integrated video data and EEG data to develop a multimodal IED detection model, resulting in a reduction of false positives by nearly one-third compared with the single-modal approach. For every EEG segment, we retrieved the video frames whose absolute wall-clock timestamps fall within the same interval, ensuring precise synchronization between the two modalities. Specifically, to avoid the interference from caregivers and medical staff, a patient detection model, YOLOv5-Patient, was used to identify the patient. Subsequently, frame difference analysis and face keypoint detection were employed to capture patient motion. Accurate and synchronous extraction of these video features can help distinguish true IEDs from artifacts, thus improving the model&#x02019;s performance in real-world clinical applications.</p><p id="Par32">To fairly assess clinical utility, evaluation should be performed using multi-center datasets independent of the training dataset. EEG data can vary significantly among centers due to multiple factors. Differences in patient demographics, such as age and underlying conditions, can impact the characteristics of the recordings. For instance, EEGs recorded in pediatric hospitals often exhibit distinct characteristics compared to those from general hospitals, reflecting differences in patient age and clinical conditions. Additionally, variations in EEG recording practices, including equipment, electrode placement, and environmental noise, can further affect data consistency. While the American Clinical Neurophysiology Society has established technical standards for EEG recordings [<xref ref-type="bibr" rid="CR26">26</xref>], which outline minimum requirements for equipment and procedures, factors such as alternating current, impedance, and patient compliance can still vary widely among institutions, introducing systematic biases that impact the model performance.</p><p id="Par33">In multi-center settings, video capturing lacks a universal technical standard, leading to variability across epilepsy centers. These differences are not limited to video resolution, camera angle, and shooting scope; they also extend to patient-related factors, such as clothing, which can vary depending on hospital policies and individual circumstances. Such discrepancies may compromise patient detection accuracy and reduce the utility of video features. To address this, we optimized the YOLOv5-Patient model [<xref ref-type="bibr" rid="CR25">25</xref>] to enhance automatic patient detection performance, ensuring its applicability across different hospital environments.</p><p id="Par34">Another challenge affecting model performance across medical centers is electrode placement variability. While the basic 19-electrode array is considered standard setup, additional electrodes, such as T1/T2 and six inferior temporal electrodes recommended by the International Federation of Clinical Neurophysiology (IFCN) [<xref ref-type="bibr" rid="CR27">27</xref>], are used depending on the patient and center. Moreover, electrode detachment due to poor patient compliance can introduce noise rather than meaningful brain activity, negatively affecting IED detection accuracy. Therefore, identifying and excluding non-reliable channels is critical before applying the EEG model. Traditionally, bad channel detection method relies on manual inspection, achieved by distribution-based statistical measures, such as channel variance and Hurst exponent [<xref ref-type="bibr" rid="CR28">28</xref>]. However, these methods may exhibit limitations in terms of accuracy and reliability. In this study, we propose a deep learning-based method for bad channel detection to improve the robustness.</p><p id="Par35">To ensure the clinical utility of IED detection, validation on multi-center external datasets is essential. Although detection models are intended for deployment across multiple hospitals and institutions, each with distinct patient populations and recording environments, multi-center validation remains absent in most IED detection studies. While some studies have incorporated multi-center data during training to enhance generalization [<xref ref-type="bibr" rid="CR29">29</xref>&#x02013;<xref ref-type="bibr" rid="CR33">33</xref>], few have validated performance on independent multi-center test datasets, leaving real-world robustness largely unverified. In this study, we conduct a large-scale prospective validation of our proposed video-EEG multimodal IED detection model using data from three epilepsy centers, providing a more comprehensive evaluation of its effectiveness.</p><p id="Par36">This paper aims to develop a robust multimodal IED detection model capable of generalizing across different medical centers. This is achieved by integrating an automatic bad channel removal method and an optimized patient detection model, both tailored to accommodate diverse clinical settings. Furthermore, different from previous studies, this paper conducts a multi-center evaluation to assess model performance in real-world applications, ensuring its practical feasibility for clinical use.</p></sec><sec id="Sec2"><title>Methods</title><p id="Par37">The proposed IED detection model is termed vEpiNetV2, which is validated on three external test datasets to conduct the multi-center study. This section details the datasets, the proposed multimodal method, and the strategies employed to address data discrepancies across epilepsy centers.</p><sec id="Sec3"><title>Dataset</title><p id="Par38">We used video-EEG data from 530 patients with and without IEDs for training. These data were collected at Peking Union Medical College Hospital (PUMCH) over the period from May 2019 to September 2023. This training dataset contains 246 h of recordings and 26,706 IEDs, including both generalized and focal discharges. The durations of most spikes/sharps ranged from 40 to 300 ms, and the duration of spike/sharp and wave complexes mostly ranged from 160 to 680 ms. All IEDs were manually annotated and reviewed by at least three EEG experts (SHY, LN, and GWF) independently.</p><p id="Par39">The test datasets were prospectively collected from three epilepsy centers from October 2023 to June 2024, containing a total of 149 patients. Specifically, 54 patients were from PUMCH (38 with IEDs), 51 from Children&#x02019;s Hospital Affiliated to Shandong University (SDQLCH, 35 with IEDs), and 44 from Beijing Tiantan Hospital (BJTTH, 34 with IEDs). For each patient, 2 to 6 h of raw video-EEG data were selected, resulting in a total of 377 h of recordings and 9232 IEDs. All the test data were initially annotated by EEG readers from the three epilepsy centers, respectively. LN and GWF subsequently reviewed and confirmed these annotations to ensure accurate test results.</p></sec><sec id="Sec4"><title>Overall framework</title><p id="Par40">Figure <xref rid="Fig1" ref-type="fig">1</xref> illustrates an overview of the proposed vEpiNetV2. This IED detection model contains two branches, EEG and video processing models. The electrical data, i.e., EEG, electrocardiogram (ECG), and electromyogram (EMG) are extracted. The epochs containing IED(s) are processed using a sliding window. The electrical data are then transformed using short-time Fourier transform (STFT) (see &#x0201c;&#x000a0;<xref rid="Sec6" ref-type="sec">EEG preprocessing</xref>&#x0201d; section). Subsequently, the bad channel removal model, ResNet-badchan, and EfficientNetV2 are applied to generate electrical features (see &#x0201c;&#x000a0;<xref rid="Sec6" ref-type="sec">Architecture of EEG network</xref>&#x0201d; section). Video data from three hospitals are processed by the patient detection model, YOLOv5-PatientV2, generating movement feature vectors through frame differences and keypoint detection (see the &#x0201c;&#x000a0;<xref rid="Sec7" ref-type="sec">Architecture of video network</xref>&#x0201d; section). Finally, the two features are fused together and the classifier makes decisions. To compare the performance of vEpiNetV2, we developed a comparative model, EpiNetV2. While EpiNetV2 shares the same architecture and training settings as vEpiNetV2, it does not include the processing of video information or features.<fig id="Fig1"><label>Fig. 1</label><caption><p>Overview of vEpiNetV2. The model contains two key components: the EEG model (left) and the video processing model (right). EEG and video data are extracted and processed to generate electrical and video feature vectors. These features are then fused together, and the classifier makes the decision based on the combined features (top). Abbreviations: IED, interictal epileptiform discharge; STFT, short-time Fourier transform</p></caption><graphic xlink:href="12916_2025_4316_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec5"><title>EEG preprocessing</title><p id="Par41">The placement of 19 electrodes (Fp2, F4, C4, P4, O2, F8, T4, T6, Fz, Cz, Pz, Fp1, F3, C3, P3, O1, F7, T3, T5) follows the standard 10&#x02013;20 system across all three epilepsy centers, supplemented by A1/A2, ECG, and EMG electrodes. Additionally, T1 and T2 electrodes are used for all patients at PUMCH and partial patients at SDQLCH and BJTTH. The EEGs are resampled to 500 Hz and filtered using a 50 Hz notch, 0.1&#x02013;70 Hz filter to eliminate interferences from alternating current power, high-frequency muscle activities, and baseline drifts. Video data were captured using Sony SNC-WR630 and SNC-EP580 cameras at PUMCH and BJTTH, while SDQLCH utilized both Sony SNC-WR630 and BOSCH AUTODOME IP4000i cameras.</p><p id="Par42">All video-EEG data was split into 4-s epochs for training and testing. To augment the training data and improve the detection of IEDs occurring near epoch boundaries, a sliding window strategy was employed. As exhibited in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, we selected a 6-s segment spanning 3 s before and after an IED. A fixed 4-s window slid across the 6-s segment with a step size of 1 s, enabling more effective feature extraction from IED epochs. Additionally, this approach increased the number of training IEDs from 26,706 to 67,592. The non-IED epochs were split into non-overlapping 4-s epochs, and the details are provided in the bottom-left corner of Fig. <xref rid="Fig1" ref-type="fig">1</xref>.</p></sec><sec id="Sec6"><title>Architecture of EEG network</title><p id="Par43">The EEG data, which contains temporal information, was then converted into two-dimensional matrices X, using STFT [<xref ref-type="bibr" rid="CR34">34</xref>]. We applied a ResNet18-based bad channel removal model, referred to as ResNet-badchan in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. ResNet-18 is a lightweight residual network, achieving high accuracy with minimal parameters. Previous research demonstrated ResNet-18&#x02019;s exceptional performance in EEG analysis [<xref ref-type="bibr" rid="CR35">35</xref>]. A bad channel is defined as an EEG electrode that is detached or unconnected, resulting in the absence of brain activity data. EEG signals from bad channels were manually annotated and split into 4-s epochs. We selected 86,906 bad-channel epochs, each containing one channel signal, alongside 716,827 epochs from normally connected channels. After being transformed by STFT, 80% of these epochs were used to train the ResNet-badchan model, while the remaining 20% were reserved for testing. The EEG data from the 19 electrodes, A1/A2, and T1/T2 electrodes were fed into ResNet-badchan before further processing. If a bad channel is detected, the corresponding signals are set to zero to eliminate any negative impact on the vEpiNetV2 detection model.</p><p id="Par44">To enhance feature extraction, we augmented the 23 original EEG channels to 77 channels (23 originals, 19 average references, 16 longitudinal, 16 transverse, 1 ECG, 2 EMG). EfficientNetV2 [<xref ref-type="bibr" rid="CR36">36</xref>], a CNN composed of fused-MBCov blocks and MBCov blocks, was employed to extract features from the matrices, yielding low-dimensional features that are easy for the classifier to use. It has superior feature extraction and multi-scale feature fusion capabilities with low latency and rapid processing speed. A 1024-dimensional feature vector is extracted by EfficientNetV2 for further use. Details of the training of ResNet-18 and EfficientNetV2 are provided in Additional file 1 Table S1.</p></sec><sec id="Sec7"><title>Architecture of video network</title><p id="Par45">We previously developed an object detection model, YOLOv5-Patient, to identify the patient undergoing EEG monitoring [<xref ref-type="bibr" rid="CR25">25</xref>]. You Only Look Once (YOLO) is developed as a single-shot object detection method [<xref ref-type="bibr" rid="CR37">37</xref>]. We choose YOLOv5, due to its superior detection accuracy and lightweight model size [<xref ref-type="bibr" rid="CR38">38</xref>]. Training details of YOLOv5 are provided in Additional file 1 Table S1. However, video data vary across hospitals, as shown in the bottom-right corner of Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Specifically, the camera angle relative to patients is most tilted in BJTTH, followed by SDQLCH, and least in PUMCH. The view scope is widest in BJTTH, followed by PUMCH, and narrowest in SDQLCH. Additionally, video frames from BJTTH often contain more than one patient. Patients in PUMCH wear uniforms, whereas those in the other two centers typically wear personal clothing.</p><p id="Par46">The YOLOv5-Patient model trained on PUMCH videos was applied to videos from the other two centers. The mean average precisions (mAPs) are 81.95% and 63.5% in SDQLCH and BJTTH, respectively. The model&#x02019;s inferior performance in these centers is possible due to the differences in patient characteristics (wearing) and the presence of multiple patients in the frame.</p><p id="Par47">To address these challenges, we manually annotated 3420 video frames from SDQLCH and 5104 frames from BJTTH, together with 12,904 previously annotated frames derived from PUMCH. Of these, 80% were used for training, and 20% were reserved for testing. The optimized model YOLOv5-PatientV2 was then integrated into vEpiNetV2.</p><p id="Par48">After detecting the patient, movement features are extracted. In some cases, patients&#x02019; bodies and limbs may be covered by quilts or clothing, making it difficult to capture keypoints accurately. To address this, we applied the frame difference method [<xref ref-type="bibr" rid="CR39">39</xref>] to measure the range of body movements, generating a feature representing the extent of body movement. Equation (<xref rid="Equ1" ref-type="disp-formula">1</xref>) shows the calculation of frame difference <italic>D</italic>, where <italic>n</italic> refers to the frame index, (<italic>x</italic>, <italic>y</italic>) refers to the point&#x02019;s location, <italic>f</italic><sub><italic>n</italic></sub>(<italic>x</italic>,<italic>y</italic>) refers to the point&#x02019;s value of the <italic>n</italic>th frame.</p><p id="Par49">
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e673">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${D_n(x,y)}\:=\vert fn(x,y)\:-\:f_{n-1}(x,y)\vert$$\end{document}</tex-math><mml:math id="d33e679" display="block"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mspace width="0.222222em"/><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>f</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mspace width="0.222222em"/><mml:mo>-</mml:mo><mml:mspace width="0.222222em"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="12916_2025_4316_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par50">The Simple Keypoints (SKPS) system (<ext-link ext-link-type="uri" xlink:href="https://github.com/open-mmlab/mmpose/tree/main/projects/skps">https://github.com/open-mmlab/mmpose/tree/main/projects/skps</ext-link>) is applied to track movements of the head, eyes, mouth, and nose [<xref ref-type="bibr" rid="CR25">25</xref>]. The aspect ratio (AR) is used to measure the movements of the eyes and mouth, following Eq. (<xref rid="Equ2" ref-type="disp-formula">2</xref>). Taking the eye as an example, width<sub>eye</sub> refers to the distance between the leftmost and rightmost eye keypoints, and height<sub>eye</sub> refers to the distance between the top and bottom eye keypoints.</p><p id="Par51">
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e747">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{AR}}_{\mathrm{eye}}\:=\:{\mathrm{width}}_{\mathrm{eye}}/{\mathrm{height}}_{\mathrm{eye}}$$\end{document}</tex-math><mml:math id="d33e753" display="block"><mml:mrow><mml:msub><mml:mi mathvariant="normal">AR</mml:mi><mml:mi mathvariant="normal">eye</mml:mi></mml:msub><mml:mspace width="0.222222em"/><mml:mo>=</mml:mo><mml:mspace width="0.222222em"/><mml:msub><mml:mi mathvariant="normal">width</mml:mi><mml:mi mathvariant="normal">eye</mml:mi></mml:msub><mml:mo stretchy="false">/</mml:mo><mml:msub><mml:mi mathvariant="normal">height</mml:mi><mml:mi mathvariant="normal">eye</mml:mi></mml:msub></mml:mrow></mml:math><graphic xlink:href="12916_2025_4316_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par52">The body movement feature derived from the frame difference, along with five AR features from the head, two eyes, mouth, and nose, forming a 6-dimensional vector. The vector is subsequently augmented to a 128-dimensional vector using a multilayer perceptron (MLP). Finally, these augmented movement features are fused with EEG features and are input into an MLP-based classifier for integrated analysis and classification.</p></sec><sec id="Sec8"><title>Test pipeline and EEG comparison</title><p id="Par53">During the test phase, the continuous video-EEG recording of each patient is divided into consecutive 4-s segments. Each EEG segment is converted into a time&#x02013;frequency tensor by STFT. This tensor first passes through ResNet&#x02013;badchan to remove bad channel(s). Data augmentation yields a 77&#x02009;&#x000d7;&#x02009;129&#x02009;&#x000d7;&#x02009;126 tensor, and then through EfficientNetV2 to extract EEG features. In parallel, the synchronised video segment is processed by YOLOv5&#x02013;patientV2, frame difference analysis, and SKPS to produce a motion feature vector. The EEG and video features are concatenated and fed to the classifier, which outputs a segment-level label of &#x0201c;IED&#x0201d; or &#x0201c;non-IED.&#x0201d;</p><p id="Par54">Prior to conducting the IED detection, we compared EEG data across three epilepsy centers to assess the inter-center variability in EEG data. To directly assess the statistical differences in EEG data across the three epilepsy centers, we randomly selected 100,000 4-s EEG epochs from each of the three epilepsy centers. The average amplitude across the 19 electrodes for each 4-s epoch was calculated. We used the violin plot [<xref ref-type="bibr" rid="CR40">40</xref>] to depict the distributions of the average amplitude for the three centers. This analysis offers a straightforward statistical comparison of the variability in EEG signals between hospitals.</p><p id="Par55">On the other hand, to explore the intrinsic differences in EEG data, we employed a deep learning approach. The EEG data from the 19 electrodes of the selected epochs were transformed into two-dimensional matrices using STFT. A 3D classifier, X3D_L, was utilized to develop a three-class classification model to validate the discriminative nature of EEG data from different hospitals. Eighty percent of the 300,000 epochs were used for training, while the remaining 20% were reserved for testing. Please note that the patients included in the test datasets did not overlap with those in the training datasets.</p></sec><sec id="Sec9"><title>Evaluation criteria</title><p id="Par56">Quantitative statistics are presented as the mean (range). Predictions are made at the 4-s segment level. Each segment generates a binary classification. A true positive (TP) is recorded when the prediction of the detection model at the 4-s segment aligns with the presence of IED(s) in this segment; otherwise, it is a false positive (FP). If the prediction is negative, it is classified as a true negative (TN) when no IED is present and a false negative (FN) if IED is present. To assess the model&#x02019;s performance in detecting IEDs across multiple centers, we used the area under the receiver operating characteristic (AUROC), along with sensitivity/recall, specificity, accuracy, and precision. These metrics are calculated as follows:</p><p id="Par57">Sensitivity/Recall&#x02009;=&#x02009;<italic>TP</italic>&#x02215;(<italic>TP</italic>&#x02009;+&#x02009;<italic>FN</italic>);</p><p id="Par58">Specificity&#x02009;=&#x02009;<italic>TN</italic>&#x02215;(<italic>TN</italic>&#x02009;+&#x02009;<italic>FP</italic>);</p><p id="Par59">Precision&#x02009;=&#x02009;<italic>TP</italic>&#x02215;(<italic>TP</italic>&#x02009;+&#x02009;<italic>FP</italic>).</p><p id="Par60">Given the imbalance in multi-center prospective test datasets, we used precision, false positives per minute, and the area under precision-recall curves (AUPRC) as primary measurements. We calculated the 95% confidence intervals (CIs) for performance measures using Wilson&#x02019;s method.</p><p id="Par61">For EEG data comparison across centers, analysis of variance (ANOVA) was performed. The three-class classification requires different evaluation metrics compared to traditional binary classification. We employ micro-averaging [<xref ref-type="bibr" rid="CR41">41</xref>] to compute average precision and AUC by the sum of <italic>TP</italic>, <italic>TN</italic>, <italic>FN</italic>, and <italic>FP</italic>, as shown in Eq. (<xref rid="Equ3" ref-type="disp-formula">3</xref>). Besides, precision, sensitivity, and AUPRC are calculated to evaluate the performance of each class.<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e850">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${M}_{\text{micro}}=M\left(\sum TP,\sum FP,\sum TN,\sum FN\right)$$\end{document}</tex-math><mml:math id="d33e856" display="block"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mtext>micro</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mfenced close=")" open="("><mml:mo>&#x02211;</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12916_2025_4316_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p></sec></sec><sec id="Sec10"><title>Results</title><sec id="Sec11"><title>Information of test datasets</title><p id="Par62">In the prospective testing, a total of 377 h of raw video-EEG data from 149 patients were included, containing 9232 IEDs. The demographic information of the patients is summarized in Table <xref rid="Tab1" ref-type="table">1</xref>. The test datasets span a wide range of age stages from infancy to elder. Children and young patients constitute the majority of the test population. The number of males slightly exceeds females across all three centers.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Demographic information of datasets and validation results in multi-centers</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="2"/><th align="left" rowspan="2"><bold>Training dataset</bold></th><th align="left" colspan="3"><bold>Test datasets</bold></th></tr><tr><th align="left" colspan="2"/><th align="left"><bold>PUMCH</bold></th><th align="left"><bold>SDQLCH</bold></th><th align="left"><bold>BJTTH</bold></th></tr></thead><tbody><tr><td align="left" colspan="2"><bold>Number</bold></td><td align="left">530</td><td align="left">54</td><td align="left">51</td><td align="left">44</td></tr><tr><td align="left" colspan="2"><bold>Age (mean, range)</bold></td><td align="left">30 (3-87)</td><td align="left">27 (9-70)</td><td align="left">9 (0.1-19)</td><td align="left">30 (9-65)</td></tr><tr><td align="left" colspan="2"><bold>Gender (M:F)</bold></td><td align="left">291:239</td><td align="left">29:25</td><td align="left">27:24</td><td align="left">24:20</td></tr><tr><td align="left" rowspan="3"><bold>Epoch</bold></td><td align="left"><bold>Sum</bold></td><td align="left">221,503</td><td align="left">127,605</td><td align="left">70,322</td><td align="left">141,335</td></tr><tr><td align="left"><bold>IEDs</bold></td><td align="left">26,706</td><td align="left">2,680</td><td align="left">3,669</td><td align="left">2,883</td></tr><tr><td align="left"><bold>Non-IEDs</bold></td><td align="left">194,797</td><td align="left">124,925</td><td align="left">66,653</td><td align="left">138,452</td></tr><tr><td align="left" rowspan="2" colspan="2"><bold>Sensitivity</bold></td><td align="left" colspan="4"><bold>Precision (PUMCH)</bold></td></tr><tr><td align="left"><bold>vEpiNetV2</bold></td><td align="left"><bold>FPR(/min)</bold></td><td align="left"><bold>EpiNetV2</bold></td><td align="left"><bold>FPR(/min)</bold></td></tr><tr><td align="left" colspan="2">&#x02003;60% (58.1%, 61.8%)</td><td align="left">79.0% (77.2%, 80.8%)</td><td align="left">0.050</td><td align="left">78.5% (76.7%, 80.3%)</td><td align="left">0.052</td></tr><tr><td align="left" colspan="2">&#x02003;65% (63.2%, 66.8%)</td><td align="left">76.1% (74.3%, 77.8%)</td><td align="left">0.064</td><td align="left">76.0% (74.2%, 77.7%)</td><td align="left">0.065</td></tr><tr><td align="left" colspan="2">&#x02003;70% (68.2%, 71.7%)</td><td align="left">71.8% (70.1%, 73.5%)</td><td align="left">0.087</td><td align="left">72.8% (71.1%, 74.5%)</td><td align="left">0.082</td></tr><tr><td align="left" colspan="2">&#x02003;75% (73.3%, 76.6%)</td><td align="left">67.5% (65.8%, 69.2%)</td><td align="left">0.114</td><td align="left">66.8% (65.1%, 68.5%)</td><td align="left">0.118</td></tr><tr><td align="left" colspan="2">&#x02003;80% (78.4%, 81.5%)</td><td align="left">61.5% (59.9%, 63.1%)</td><td align="left">0.158</td><td align="left">58.5% (56.9%, 60.1%)</td><td align="left">0.178</td></tr><tr><td align="left" colspan="2">&#x02003;85% (83.6%, 86.4%)</td><td align="left">53.7% (52.2%, 55.2%)</td><td align="left">0.231</td><td align="left">48.7% (47.3%, 50.1%)</td><td align="left">0.282</td></tr><tr><td align="left" colspan="2">&#x02003;90% (88.8%, 91.1%)</td><td align="left">41.4% (40.1%, 42.7%)</td><td align="left">0.401</td><td align="left">32.8% (31.8%, 33.9%)</td><td align="left">0.580</td></tr><tr><td align="left" rowspan="2" colspan="2"><bold>Sensitivity</bold></td><td align="left" colspan="4"><bold>Precision (SDQLCH)</bold></td></tr><tr><td align="left"><bold>vEpiNetV2</bold></td><td align="left"><bold>FPR(/min)</bold></td><td align="left"><bold>EpiNetV2</bold></td><td align="left"><bold>FPR(/min)</bold></td></tr><tr><td align="left" colspan="2">&#x02003;60% (58.4%, 61.6%)</td><td align="left">86.1% (84.7%, 87.4%)</td><td align="left">0.076</td><td align="left">85.1% (83.8%, 86.5%)</td><td align="left">0.082</td></tr><tr><td align="left" colspan="2">&#x02003;65% (63.5%, 66.5%)</td><td align="left">84.2% (82.9%, 85.6%)</td><td align="left">0.095</td><td align="left">82.0% (80.6%, 83.4%)</td><td align="left">0.112</td></tr><tr><td align="left" colspan="2">&#x02003;70% (68.5%, 71.5%)</td><td align="left">80.4% (79.1%, 81.8%)</td><td align="left">0.133</td><td align="left">78.6% (77.2%, 80.0%)</td><td align="left">0.149</td></tr><tr><td align="left" colspan="2">&#x02003;75% (73.6%, 76.4%)</td><td align="left">74.1% (72.7%, 75.6%)</td><td align="left">0.205</td><td align="left">72.2% (70.8%, 73.6%)</td><td align="left">0.226</td></tr><tr><td align="left" colspan="2">&#x02003;80% (78.7%, 81.3%)</td><td align="left">66.9% (65.5%, 68.3%)</td><td align="left">0.310</td><td align="left">63.6% (62.2%, 65.0%)</td><td align="left">0.358</td></tr><tr><td align="left" colspan="2">&#x02003;85% (83.8%, 86.2%)</td><td align="left">57.1% (55.8%, 58.4%)</td><td align="left">0.500</td><td align="left">52.5% (51.3%, 53.8%)</td><td align="left">0.600</td></tr><tr><td align="left" colspan="2">&#x02003;90% (89.0%, 91.0%)</td><td align="left">38.3% (37.3%, 39.3%)</td><td align="left">1.134</td><td align="left">31.7% (30.8%, 32.6%)</td><td align="left">1.518</td></tr><tr><td align="left" rowspan="2" colspan="2"><bold>Sensitivity</bold></td><td align="left" colspan="4"><bold>Precision (BJTTH)</bold></td></tr><tr><td align="left"><bold>vEpiNetV2</bold></td><td align="left"><bold>FPR(/min)</bold></td><td align="left"><bold>EpiNetV2</bold></td><td align="left"><bold>FPR(/min)</bold></td></tr><tr><td align="left" colspan="2">&#x02003;60% (58.2%, 61.8%)</td><td align="left">79.3% (77.6%, 81.0%)</td><td align="left">0.048</td><td align="left">78.3% (76.6%, 80.0%)</td><td align="left">0.051</td></tr><tr><td align="left" colspan="2">&#x02003;65% (63.3%, 66.7%)</td><td align="left">74.3% (72.6%, 76.0%)</td><td align="left">0.069</td><td align="left">71.6% (69.9%, 73.4%)</td><td align="left">0.079</td></tr><tr><td align="left" colspan="2">&#x02003;70% (68.3%, 71.7%)</td><td align="left">69.2% (67.5%, 70.9%)</td><td align="left">0.095</td><td align="left">65.6% (63.9%, 67.2%)</td><td align="left">0.113</td></tr><tr><td align="left" colspan="2">&#x02003;75% (73.4%, 76.6%)</td><td align="left">61.8% (60.2%, 63.4%)</td><td align="left">0.142</td><td align="left">59.4% (57.8%, 61.0%)</td><td align="left">0.157</td></tr><tr><td align="left" colspan="2">&#x02003;80% (78.5%, 81.5%)</td><td align="left">53.2% (51.8%, 54.7%)</td><td align="left">0.215</td><td align="left">51.3% (49.8%, 52.8%)</td><td align="left">0.232</td></tr><tr><td align="left" colspan="2">&#x02003;85% (83.7%, 86.3%)</td><td align="left">43.7% (42.4%, 45.0%)</td><td align="left">0.335</td><td align="left">42.2% (41.0%, 43.5%)</td><td align="left">0.356</td></tr><tr><td align="left" colspan="2">&#x02003;90% (88.9%, 91.1%)</td><td align="left">30.3% (29.3%, 31.2%)</td><td align="left">0.635</td><td align="left">31.0% (30.0%, 32.0%)</td><td align="left">0.613</td></tr></tbody></table></table-wrap></p><p id="Par63">In the test datasets, 21 patients presented with generalized IEDs: 10 in PUMCH, 6 in SDQLCH, and 5 in BJTTH. Focal IEDs were observed in 28 patients from PUMCH (temporal 17, frontal 6, central 1, posterior head 2, multiple 2), 29 from SDQLCH (temporal 10, frontal 8, central/parietal 5, occipital 4, multiple 2), and 29 from BJTTH (temporal 16, frontal 7, occipital 2, multiple 4).</p></sec><sec id="Sec12"><title>Display of vEpiNetV2 detection</title><p id="Par64">Sample detections across three epilepsy centers are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, including generalized and focal IEDs enlarged within black squares. The vertical line denotes the presence of an IED within a 2-s window before and after the marked position, which is demarcated by a red dotted line. At the base of the detection line, an AI prediction confidence score is exhibited (maximum at 1.000), with the text enlarged in a red circle.<fig id="Fig2"><label>Fig. 2</label><caption><p>Sample detections of vEpiNetV2. The vertical line denotes the presence of an IED within a 2-s window (red dotted line) before and after the marked position. AI prediction confidences are placed at the base of the detection lines (maximum at 1.000), with the text enlarged in a red circle. <bold>a</bold> and <bold>d</bold> Temporal and central IEDs in PUMCH. <bold>b</bold> and <bold>e</bold> Generalized and temporal IEDs in SDQLCH. <bold>c</bold> and <bold>f</bold> Generalized fast rhythm and frontal IED in BJTTH. Abbreviations: IED, interictal epileptiform discharge; PUMCH, Peking Union Medical College Hospital; SDQLCH, Children&#x02019;s Hospital Affiliated to Shandong University; BJTTH, Beijing Tiantan Hospital</p></caption><graphic xlink:href="12916_2025_4316_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec13"><title>EEG discrepancy across epilepsy centers</title><p id="Par65">The violin plot results indicate differences in amplitude distribution across the three epilepsy centers (Fig. <xref rid="Fig3" ref-type="fig">3</xref>a). The mean amplitudes are&#x02009;&#x02212;&#x02009;6.47 &#x003bc;V (SD 22.44) in PUMCH, 2.63 &#x003bc;V (SD 28.60) in SDQLCH, and&#x02009;&#x02212;&#x02009;1.51 &#x003bc;V (SD 18.22) in BJTTH. As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, EEG data in SDQLCH exhibits higher amplitude activities compared to the other two centers, which may be due to the predominance of child patients. We compared the amplitude difference in three centers using ANOVA, resulting in a significant difference (<italic>P</italic>&#x02009;&#x0003c;&#x02009;0.001). Post hoc analyses with Bonferroni correction confirm significant differences in all three pairwise comparisons (<italic>P</italic><sub>PUMCH-SDQLCH</sub>&#x02009;&#x0003c;&#x02009;0.001; <italic>P</italic><sub>PUMCH-BJTTH</sub>&#x02009;&#x0003c;&#x02009;0.001; <italic>P</italic><sub>SDQLCH-BJTTH</sub>&#x02009;&#x0003c;&#x02009;0.001).<fig id="Fig3"><label>Fig. 3</label><caption><p>The discrepancy of EEG data in the three epilepsy centers. <bold>a</bold> The violin plot illustrates the amplitude distributions, highlighting differences across centers. <bold>b </bold>and <bold>c</bold> Performance of an X3D_L-based classification model in distinguishing EEG data by centers. Abbreviations: PUMCH, Peking Union Medical College Hospital; SDQLCH, Children&#x02019;s Hospital Affiliated to Shandong University; BJTTH, Beijing Tiantan Hospital</p></caption><graphic xlink:href="12916_2025_4316_Fig3_HTML" id="MO3"/></fig></p><p id="Par66">We further evaluated the performance of the X3D_L three-class classification using fivefold cross-validation. The average sensitivity and precision are 76.5% at the Break-Even Point. The average AUPRC is 0.86, with center-specific AUPRC values of 0.92 in PUMCH, 0.71 in BJTTH, and 0.88 in SDQLCH (Fig. <xref rid="Fig3" ref-type="fig">3</xref>b). The values of AUC in each center are 0.93 in PUMCH, 0.86 in BJTTH, and 0.94 in SDQLCH (Fig. <xref rid="Fig3" ref-type="fig">3</xref>c). These results demonstrate the EEG discrepancy among different epilepsy centers.</p></sec><sec id="Sec14"><title>Multi-center validation</title><p id="Par67">Using the same training parameter settings, vEpiNetV2 showed superior performance in IED detection compared with EpiNetV2 across all three centers. The effect of the video feature is most notable in the PUMCH center, with AUPRC increasing from 0.7246 to 0.7595. Similar improvements were observed in BJTTH and SDQLCH, with AUPRC increasing from 0.7356 to 0.7585 and 0.7766 to 0.7994, respectively (Fig. <xref rid="Fig4" ref-type="fig">4</xref>a&#x02013;c). At a sensitivity of approximately 80%, vEpiNetV2 achieved a precision of over 50% and a false positive rate (FPR, per min) of less than 0.3 in all three centers. The improvement in precision achieved most at a sensitivity of 90% in PUMCH (41.4% vs 32.8%) and SDQLCH (38.3% vs 31.7%). For BJTTH, the most significant precision improvement occurred at a sensitivity near 70% (69.2% vs 65.6%) (Table <xref rid="Tab1" ref-type="table">1</xref> and Additional file 1 Table S2-S4). When the data from all three centers are put together, the video feature is demonstrated to be especially effective at reducing false positives at higher sensitivity levels. At a sensitivity of 95%, vEpiNetV2 reduced false positives by 24% compared to EpiNetV2.<fig id="Fig4"><label>Fig. 4</label><caption><p>Multi-center validation results. Receiver operating characteristic curves of model performance across three epilepsy centers (PUMCH, SDQLCH, BJTTH) are presented. <bold>a</bold>&#x02013;<bold>c</bold> Precision-sensitivity/sensitivity curves of vEpiNetV2 and four comparative models. <bold>d</bold>&#x02013;<bold>f</bold> Specificity-sensitivity curves of the vEpiNetV2 and comparative models. Abbreviations: AUPRC: area under the precision-recall curve; AUC: area under the curve; PUMCH, Peking Union Medical College Hospital; SDQLCH, Children&#x02019;s Hospital Affiliated to Shandong University; BJTTH, Beijing Tiantan Hospital</p></caption><graphic xlink:href="12916_2025_4316_Fig4_HTML" id="MO4"/></fig></p><p id="Par68">At a sensitivity of 90%, the specificities of vEpiNetV2 exceed above 90% across all centers (Additional file 1 Tables S5&#x02013;S7). The AUCs of vEpiNetV2 are consistently higher than those of EpiNetV2 in all centers, indicating overall improved performance (Fig <xref rid="Fig4" ref-type="fig">4</xref>d&#x02013;f).</p></sec><sec id="Sec15"><title>Effectiveness of bad channel removing</title><p id="Par69">The bad channel removal model, ResNet-badchan, achieved an mAP of 90.2% using fivefold cross validation. It detected 650 bad channel epochs in PUMCH, 4041 epochs in SDQLCH, and 3000 epochs in BJTTH. Among the detected bad channels, 95.2% in SDQLCH and 82.4% in BJTTH were from the T1 and T2 electrodes. These two electrodes were not placed on all patients in the two centers, but the signals from the environment were recorded. In order to assess the effectiveness of ResNet-badchan, we developed a comparative model without ResNet-badchan, named badchan-vEpiNetV2. In contrast, badchan-EpiNetV2 represents the EEG model without bad channel removal.</p><p id="Par70">Figure <xref rid="Fig4" ref-type="fig">4</xref> illustrates the performance comparisons on the multi-center prospective test dataset. The results demonstrate the effectiveness of bad channel removal across all three centers, with the most significant improvement observed in SDQLCH, followed by BJTTH. For the test dataset from PUMCH, the improvement of bad channel removal is slight (Additional file 1 Tables S2&#x02013;S7). These findings suggest that the artifacts from T1/T2 electrodes impact the performance most. In PUMCH, T1/T2 electrodes were consistently placed. The amplitudes of most interferent signals from the unconnected T1/T2 electrodes are very low in BJTTH (Fig. <xref rid="Fig2" ref-type="fig">2</xref>f), but fluctuate a lot in SDQLCH (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b). Additionally, we found that incorporating video features could mitigate the impact of bad channels. Particularly for the BJTTH dataset, the precision and specificity values of Badchan-vEpiNetV2 were very close to those of vEpiNetV2.</p></sec><sec id="Sec16"><title>Effectiveness of YOLOv5-PatientV2</title><p id="Par71">The YOLOv5-PatientV2 model demonstrated higher mAP and improved patient detection accuracy in SDQLCH (from 81.95% to 92.47%) and BJTTH (from 63.5% to 87.12%), compared to YOLOv5-Patient. To evaluate its impact on IED detection, we developed a comparative IED detection model, preYOLOv5-Patient, which uses YOLO-Patient for patient detection.</p><p id="Par72">The comparative evaluation of IED detection is shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. In PUMCH, vEpiNetV2 showed only a slight improvement over preYOLO-vEpiNetV2. The effectiveness of YOLOv5-PatientV2 was most evident in BJTTH, increasing the AUPRC from 0.6667 to 0.7585, followed by SDQLCH, increasing AUPRC from 0.7716 to 0.7994, especially at high sensitivities. The precision and specificity across different sensitivity levels are presented in Additional file 1 Tables S2&#x02013;S7. As shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, inaccurate patient detection could have a negative impact on the IED detection, compared to EpiNetV2.</p></sec></sec><sec id="Sec17"><title>Discussion</title><p id="Par73">In this study, we developed a multi-modal deep learning-based model, vEpiNetV2, for IEDs detections based on EEG and video data. The model demonstrated good performance in identifying IEDs, achieving favorable AUC values, along with high sensitivity and specificity. Moreover, the present study demonstrated precisions exceeding more than 50% and specificities more than 97% at the sensitivity of 80%, with false positive rates of less than 0.3 per minute. These results were consistent not only in the PUMCH validation dataset but also in the other two large-scale external validation datasets. The robustness and generalizability of the vEpiNetV2 were validated in this multi-center, prospective study. Additionally, the video features were demonstrated to be able to improve model performance across all three centers.</p><p id="Par74">Previous studies have developed IED detection models to achieve accurate automatic IED annotation in clinical practice. However, their clinical applicability was unclear since most studies had a small sample size of test data without validation in multi-center datasets. Most studies relied on internal validation methods such as cross-validation or Leave-One-Patient-Out/Leave-One-Institution-Out testing [<xref ref-type="bibr" rid="CR42">42</xref>]. While several studies were validated on external datasets, there was often a small sample of IEDs (54&#x02013;973 IED labels) [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR43">43</xref>, <xref ref-type="bibr" rid="CR44">44</xref>]. Other studies used independent test datasets containing more than 1,000 IEDs. For instance, Louren&#x000e7;o et al. tested the model on the test datasets containing more than 30 patients, with one test dataset including 1918 IEDs and achieving a false positive rate of 3.05 [<xref ref-type="bibr" rid="CR45">45</xref>]. In our previous study, we validated the model on a dataset with 215 h of video-EEG data and 2349 IEDs from 50 patients, achieving an AUC above 0.98, an AUPRC of 0.86, and a false positive rate of 0.045 (sensitivity 80%) [<xref ref-type="bibr" rid="CR25">25</xref>]. However, these models were evaluated in a single center, overlooking the importance of robustness and generalizability. In this study, we validated our model on test data from three epilepsy centers, which presents visible heterogeneity and significant differences that could be classified by a deep learning-based classification model. These results suggest that multi-center validation is necessary for evaluating the generalization and robustness of IED detection models. The current study demonstrated moderate performance (AUPRCs 0.76&#x02013;0.79, AUCs 0.96&#x02013;0.98) on a large-scale prospective test dataset comprising 377 h of raw EEG data containing 9232 IEDs from 149 patients. This provides a more comprehensive validation.</p><p id="Par75">The test data in the present study consists of raw, consecutive video-EEG data without any manual processing, reflecting real-world clinical applications. The test dataset is highly imbalanced, with the percentage of IEDs less than 3%. Our study shows that precisions calculated from the false positives and AUPRC can serve as a reliable metric for comparing different models. In contrast, specificity and AUC, which tend to have very high values, are less useful for comparative evaluation and show little correlation with false positives. Precision reflects the proportion of true positive IEDs among all detected events, while sensitivity indicates the model&#x02019;s ability to identify true IEDs, both of which are critical metrics for neurologists reviewing and confirming IEDs. Most studies used specificity and sensitivity for evaluation, and only a few considered precision [<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR46">46</xref>, <xref ref-type="bibr" rid="CR47">47</xref>]. In cross-validation of the balanced test dataset, the precision is close to the specificity [<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR30">30</xref>]. However, in real-world imbalanced datasets, as demonstrated in our study, precision and sensitivity provide a more accurate reflection of model performance.</p><p id="Par76">Synchronized video during EEG recordings could monitor patients&#x02019; physical actions and habitual events. Besides the paroxysmal events, video helps to differentiate the movement artifacts and neuron electrical signals. We previously proved that capturing the features of physical actions using deep learning algorithms on video data can reduce false positives by nearly one-third [<xref ref-type="bibr" rid="CR25">25</xref>]. However, when involving multiple centers, the patient detection model trained on a single center failed to generalize to other centers. This is likely due to differences in patient clothing and video settings: PUMCH patients wear hospital gowns and are centered in the video frame, while patients in the other two centers wear personal clothing, and some frames contain multiple patients. The failure of patient detection negatively impacts overall IED detection, reducing the AUPRC from 0.76 to 0.67 in BJTTH and from 0.80 to 0.77 in SDQLCH. After training on multi-center video data, the YOLOv5-PatientV2 achieved a mAP of 91.2% for patient recognition. The video features improved the performance of IED detection, increasing the AUPRC by 0.023&#x02013;0.035. There are no criteria for video camera capturing. To improve the robustness of the video model, the patients&#x02019; wearing, camera angle, and shooting scope should be standardized.</p><p id="Par77">Additionally, this present study exhibited the impact of bad channels on IED detection. The training data of vEpiNetV2 included 19 electrodes and T1/T2 electrodes. Unconnected T1/T2 electrodes often recorded environmental artifact signals, significantly affecting model performance. Detecting these bad channels using ResNet-badchan and subsequently setting them to zero can significantly improve the performance. Furthermore, we also found that video data can compensate for the negative impact of bad channels. Validation on the PUMCH test dataset showed that a small number of detached or unconnected electrodes (like patients left for examinations) had minimal effect on overall performance.</p></sec><sec id="Sec18"><title>Conclusions</title><p id="Par78">In conclusion, we developed vEpiNetV2, a deep learning-based multimodal model for IED detection by integrating video and EEG data. The performance, generalizability, and robustness of vEpiNetV2 were validated in this multi-center study, demonstrating its ability to adapt to different datasets from multiple centers. These findings highlight the model&#x02019;s potential for improving the accuracy of IED detection in clinical practice through the incorporation of video features, even in diverse clinical environments. Besides, our results indicate that detection accuracy could be further improved by a hierarchical strategy that first determines whether an IED is generalized or focal and then applies a network tailored to that specific pattern, which may promise higher sensitivity for focal events while maintaining the current high accuracy for generalized discharges.</p></sec><sec id="Sec19" sec-type="supplementary-material"><title>Supplementary Information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="12916_2025_4316_MOESM1_ESM.docx"><caption><p>Additional file 1: Table S1-Training details of deep learning models. Table S2-Precisions across sensitivity in different models in PUMCH. Table S3-Precisions across sensitivity in different models in SDQLCH. Table S4-Precisions across sensitivity in different models in BJTTH. Table S5-Specificity across sensitivity in different models in PUMCH. Table S6-Specificity across sensitivity in different models in SDQLCH. Table S7-Specificity across sensitivity in different models in BJTTH.</p></caption></media></supplementary-material></p></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>IED</term><def><p id="Par5">Interictal epileptiform discharge</p></def></def-item><def-item><term>EEG</term><def><p id="Par6">Electroencephalogram</p></def></def-item><def-item><term>CNN</term><def><p id="Par7">Convolutional neural network</p></def></def-item><def-item><term>IFCN</term><def><p id="Par8">International Federation of Clinical Neurophysiology</p></def></def-item><def-item><term>PUMCH</term><def><p id="Par9">Peking Union Medical College Hospital</p></def></def-item><def-item><term>SDQLCH</term><def><p id="Par10">Children&#x02019;s Hospital Affiliated to Shandong University</p></def></def-item><def-item><term>BJTTH</term><def><p id="Par11">Beijing Tiantan Hospital</p></def></def-item><def-item><term>ECG</term><def><p id="Par12">Electrocardiogram</p></def></def-item><def-item><term>EMG</term><def><p id="Par13">Electromyogram</p></def></def-item><def-item><term>STFT</term><def><p id="Par14">Short-time Fourier transform</p></def></def-item><def-item><term>YOLO</term><def><p id="Par15">You Only Look Once</p></def></def-item><def-item><term>mAP</term><def><p id="Par16">Mean average precision</p></def></def-item><def-item><term>SKPS</term><def><p id="Par17">Simple Keypoints</p></def></def-item><def-item><term>AR</term><def><p id="Par18">Aspect ratio</p></def></def-item><def-item><term>MLP</term><def><p id="Par19">Multilayer perceptron</p></def></def-item><def-item><term>TP</term><def><p id="Par20">True positive</p></def></def-item><def-item><term>FP</term><def><p id="Par21">False positive</p></def></def-item><def-item><term>TN</term><def><p id="Par22">True negative</p></def></def-item><def-item><term>FN</term><def><p id="Par23">False negative</p></def></def-item><def-item><term>AUROC</term><def><p id="Par24">Area under the receiver operating characteristic</p></def></def-item><def-item><term>AUPRC</term><def><p id="Par25">The area under precision-recall curves</p></def></def-item><def-item><term>CI</term><def><p id="Par26">Confidence interval</p></def></def-item><def-item><term>ANOVA</term><def><p id="Par27">Analysis of variance</p></def></def-item></def-list></glossary><fn-group><fn><p><bold>Publisher&#x02019;s Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The authors thank all participants who contributed to this study and the clinical staff for their assistance during data collection.</p></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>N.L., Q.L., L.C., X.S., and Z.G. designed the study and developed the conceptual ideas. N.L., W.G., H.S., Q.L., F.Q., L.W., and S.W. collected and annotated the data. L.L., P.H., Z.L., H.H., and Y.D. implemented the main algorithms and other computational analyses. N.L., L.L., P.H., and G.Y. analyzed the results. N.L. and G.Y. wrote the manuscript. Q.L., L.C., X.S., and Z.G. reviewed the manuscript. Q.L. had final responsibility for the decision to submit for publication. All authors read and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This study was supported by National Key Research and Development Project (grant number: 2022YFC2503800), CAMS Innovation Fund for Medical Sciences (CIFMS) (grant number: 2023-I2M-C&#x00026;T-B-023), Big Data Science and Technology Project of Jinan Municipal Health Commission (grant number: 2022-YBD-17), Science and Technology Program of the Joint Fund of Scientific Research for the Public Hospitals of Inner Mongolia Academy of Medical Sciences (grant number: 2024GLLH0452).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets used in the current study are not publicly available due to the identifiable patient information. The de-identifiable data (i.e., EEG) are available from the corresponding author on reasonable request. A data use agreement will be required before the data release and institutional review board approval as appropriate. The underlying code used to develop and validate vEpiNetV2 can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/vepiset/vEpiNetV2">https://github.com/vepiset/vEpiNetV2</ext-link>.</p></notes><notes><title>Declarations</title><notes id="FPar1"><title>Ethics approval and consent to participate</title><p id="Par79">This study was approved by the research ethics committees of Peking Union Medical College Hospital (I-22PJ169), Children&#x02019;s Hospital Affiliated to Shandong University (SDFE-IRB/MP-2024010-01), and Beijing Tiantan Hospital (KY2025-013-01). Patients had provided written informed assent for the Collection and Application of Clinical Sample and Medical Data certified and approved by the ethics committees.</p></notes><notes id="FPar2"><title>Consent for publication</title><p id="Par80">Patients had provided written informed consent for the collection, application, and non-commercial publication of de-identified clinical samples and medical data on their hospital admission. The consent is certified and approved by the research ethics committees of Peking Union Medical College Hospital.</p></notes><notes id="FPar3" notes-type="COI-statement"><title>Competing interests</title><p id="Par81">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>RS</given-names></name><name><surname>Cross</surname><given-names>JH</given-names></name><name><surname>D'Souza</surname><given-names>C</given-names></name><name><surname>French</surname><given-names>JA</given-names></name><name><surname>Haut</surname><given-names>SR</given-names></name><name><surname>Higurashi</surname><given-names>N</given-names></name><etal/></person-group><article-title>Instruction manual for the ILAE 2017 operational classification of seizure types</article-title><source>Epilepsia</source><year>2017</year><volume>58</volume><issue>4</issue><fpage>531</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1111/epi.13671</pub-id><?supplied-pmid 28276064?><pub-id pub-id-type="pmid">28276064</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Fisher RS, Cross JH, D&#x02019;Souza C, French JA, Haut SR, Higurashi N, et al. Instruction manual for the ILAE 2017 operational classification of seizure types. Epilepsia. 2017;58(4):531&#x02013;42. 10.1111/epi.13671.<pub-id pub-id-type="pmid">28276064</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Jirsa</surname><given-names>VK</given-names></name><name><surname>Stacey</surname><given-names>WC</given-names></name><name><surname>Quilichini</surname><given-names>PP</given-names></name><name><surname>Ivanov</surname><given-names>AI</given-names></name><name><surname>Bernard</surname><given-names>C</given-names></name></person-group><article-title>On the nature of seizure dynamics</article-title><source>Brain</source><year>2014</year><volume>137</volume><issue>Pt 8</issue><fpage>2210</fpage><lpage>2230</lpage><pub-id pub-id-type="doi">10.1093/brain/awu133</pub-id><?supplied-pmid 24919973?><pub-id pub-id-type="pmid">24919973</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Jirsa VK, Stacey WC, Quilichini PP, Ivanov AI, Bernard C. On the nature of seizure dynamics. Brain. 2014;137(Pt 8):2210&#x02013;30. 10.1093/brain/awu133.<pub-id pub-id-type="pmid">24919973</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">WHO. Epilepsy: a public health imperative. Geneva: World Health Organization.2019. Available from <ext-link ext-link-type="uri" xlink:href="https://iris.who.int/handle/10665/325293">https://iris.who.int/handle/10665/325293</ext-link>.</mixed-citation></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Tatum</surname><given-names>WO</given-names></name><name><surname>Rubboli</surname><given-names>G</given-names></name><name><surname>Kaplan</surname><given-names>PW</given-names></name><name><surname>Mirsatari</surname><given-names>SM</given-names></name><name><surname>Radhakrishnan</surname><given-names>K</given-names></name><name><surname>Gloss</surname><given-names>D</given-names></name><etal/></person-group><article-title>Clinical utility of EEG in diagnosing and monitoring epilepsy in adults</article-title><source>Clin Neurophysiol</source><year>2018</year><volume>129</volume><issue>5</issue><fpage>1056</fpage><lpage>1082</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2018.01.019</pub-id><?supplied-pmid 29483017?><pub-id pub-id-type="pmid">29483017</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Tatum WO, Rubboli G, Kaplan PW, Mirsatari SM, Radhakrishnan K, Gloss D, et al. Clinical utility of EEG in diagnosing and monitoring epilepsy in adults. Clin Neurophysiol. 2018;129(5):1056&#x02013;82. 10.1016/j.clinph.2018.01.019.<pub-id pub-id-type="pmid">29483017</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Kural</surname><given-names>MA</given-names></name><name><surname>Duez</surname><given-names>L</given-names></name><name><surname>Sejer Hansen</surname><given-names>V</given-names></name><name><surname>Larsson</surname><given-names>PG</given-names></name><name><surname>Rampp</surname><given-names>S</given-names></name><name><surname>Schulz</surname><given-names>R</given-names></name><etal/></person-group><article-title>Criteria for defining interictal epileptiform discharges in EEG: A clinical validation study</article-title><source>Neurology</source><year>2020</year><volume>94</volume><issue>20</issue><fpage>e2139</fpage><lpage>e2147</lpage><pub-id pub-id-type="doi">10.1212/wnl.0000000000009439</pub-id><?supplied-pmid 32321764?><pub-id pub-id-type="pmid">32321764</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Kural MA, Duez L, Sejer Hansen V, Larsson PG, Rampp S, Schulz R, et al. Criteria for defining interictal epileptiform discharges in EEG: A clinical validation study. Neurology. 2020;94(20):e2139&#x02013;47. 10.1212/wnl.0000000000009439.<pub-id pub-id-type="pmid">32321764</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>S</given-names></name><name><surname>Hao</surname><given-names>X</given-names></name><name><surname>Duan</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Zhou</surname><given-names>D</given-names></name><etal/></person-group><article-title>Epilepsy centers in China: current status and ways forward</article-title><source>Epilepsia</source><year>2021</year><volume>62</volume><issue>11</issue><fpage>2640</fpage><lpage>2650</lpage><pub-id pub-id-type="doi">10.1111/epi.17058</pub-id><?supplied-pmid 34510417?><pub-id pub-id-type="pmid">34510417</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Lin Y, Hu S, Hao X, Duan L, Wang W, Zhou D, et al. Epilepsy centers in China: current status and ways forward. Epilepsia. 2021;62(11):2640&#x02013;50. 10.1111/epi.17058.<pub-id pub-id-type="pmid">34510417</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>G</given-names></name><name><surname>Nourani</surname><given-names>M</given-names></name><name><surname>Harvey</surname><given-names>J</given-names></name></person-group><article-title>SEEG-based bilateral seizure network analysis for neurostimulation treatment</article-title><source>IEEE Trans Neural Syst Rehabil Eng</source><year>2025</year><pub-id pub-id-type="doi">10.1109/tnsre.2025.3534121</pub-id><?supplied-pmid 40266871?><pub-id pub-id-type="pmid">40266871</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Peng G, Nourani M, Harvey J. SEEG-based bilateral seizure network analysis for neurostimulation treatment. IEEE Trans Neural Syst Rehabil Eng. 2025. 10.1109/tnsre.2025.3534121.<pub-id pub-id-type="pmid">40266871</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>G</given-names></name><name><surname>Nourani</surname><given-names>M</given-names></name><name><surname>Harvey</surname><given-names>J</given-names></name><name><surname>Dave</surname><given-names>H</given-names></name></person-group><article-title>Personalized EEG feature selection for low-complexity seizure monitoring</article-title><source>Int J Neural Syst</source><year>2021</year><volume>31</volume><issue>8</issue><fpage>2150018</fpage><pub-id pub-id-type="doi">10.1142/s0129065721500180</pub-id><?supplied-pmid 33752579?><pub-id pub-id-type="pmid">33752579</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Peng G, Nourani M, Harvey J, Dave H. Personalized EEG feature selection for low-complexity seizure monitoring. Int J Neural Syst. 2021;31(8):2150018. 10.1142/s0129065721500180.<pub-id pub-id-type="pmid">33752579</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Halford</surname><given-names>JJ</given-names></name></person-group><article-title>Computerized epileptiform transient detection in the scalp electroencephalogram: obstacles to progress and the example of computerized ECG interpretation</article-title><source>Clin Neurophysiol</source><year>2009</year><volume>120</volume><issue>11</issue><fpage>1909</fpage><lpage>1915</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2009.08.007</pub-id><?supplied-pmid 19836303?><pub-id pub-id-type="pmid">19836303</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Halford JJ. Computerized epileptiform transient detection in the scalp electroencephalogram: obstacles to progress and the example of computerized ECG interpretation. Clin Neurophysiol. 2009;120(11):1909&#x02013;15. 10.1016/j.clinph.2009.08.007.<pub-id pub-id-type="pmid">19836303</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Carrie</surname><given-names>J</given-names></name></person-group><article-title>A hybrid computer technique for detecting sharp EEG transients</article-title><source>Electroencephalogr Clin Neurophysiol</source><year>1972</year><volume>33</volume><issue>3</issue><fpage>336</fpage><lpage>338</lpage><?supplied-pmid 4114920?><pub-id pub-id-type="pmid">4114920</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Carrie J. A hybrid computer technique for detecting sharp EEG transients. Electroencephalogr Clin Neurophysiol. 1972;33(3):336&#x02013;8.<pub-id pub-id-type="pmid">4114920</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname><given-names>F</given-names></name><name><surname>Schaft</surname><given-names>EV</given-names></name><name><surname>Huiskamp</surname><given-names>G</given-names></name><name><surname>Aarnoutse</surname><given-names>EJ</given-names></name><name><surname>Van't Klooster</surname><given-names>MA</given-names></name><name><surname>Krayenb&#x000fc;hl</surname><given-names>N</given-names></name><etal/></person-group><article-title>Robust compression and detection of epileptiform patterns in ECoG using a real-time spiking neural network hardware framework</article-title><source>Nat Commun</source><year>2024</year><volume>15</volume><issue>1</issue><fpage>3255</fpage><pub-id pub-id-type="doi">10.1038/s41467-024-47495-y</pub-id><?supplied-pmid 38627406?><pub-id pub-id-type="pmid">38627406</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Costa F, Schaft EV, Huiskamp G, Aarnoutse EJ, Van&#x02019;t Klooster MA, Krayenb&#x000fc;hl N, et al. Robust compression and detection of epileptiform patterns in ECoG using a real-time spiking neural network hardware framework. Nat Commun. 2024;15(1):3255. 10.1038/s41467-024-47495-y.<pub-id pub-id-type="pmid">38627406</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Abdi-Sargezeh</surname><given-names>B</given-names></name><name><surname>Shirani</surname><given-names>S</given-names></name><name><surname>Sanei</surname><given-names>S</given-names></name><name><surname>Took</surname><given-names>CC</given-names></name><name><surname>Geman</surname><given-names>O</given-names></name><name><surname>Alarcon</surname><given-names>G</given-names></name><etal/></person-group><article-title>A review of signal processing and machine learning techniques for interictal epileptiform discharge detection</article-title><source>Comput Biol Med</source><year>2024</year><volume>168</volume><fpage>107782</fpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.107782</pub-id><?supplied-pmid 38070202?><pub-id pub-id-type="pmid">38070202</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Abdi-Sargezeh B, Shirani S, Sanei S, Took CC, Geman O, Alarcon G, et al. A review of signal processing and machine learning techniques for interictal epileptiform discharge detection. Comput Biol Med. 2024;168:107782. 10.1016/j.compbiomed.2023.107782.<pub-id pub-id-type="pmid">38070202</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other"> Saeidi M, Karwowski W, Farahani FV, Fiok K, Taiar R, Hancock PA, et al. Neural Decoding of EEG Signals with Machine Learning: A Systematic Review. Brain Sci. 2021;11(11). 10.3390/brainsci11111525.</mixed-citation></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Scheuer</surname><given-names>ML</given-names></name><name><surname>Bagic</surname><given-names>A</given-names></name><name><surname>Wilson</surname><given-names>SB</given-names></name></person-group><article-title>Spike detection: inter-reader agreement and a statistical Turing test on a large data set</article-title><source>Clin Neurophysiol</source><year>2017</year><volume>128</volume><issue>1</issue><fpage>243</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2016.11.005</pub-id><?supplied-pmid 27913148?><pub-id pub-id-type="pmid">27913148</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Scheuer ML, Bagic A, Wilson SB. Spike detection: inter-reader agreement and a statistical Turing test on a large data set. Clin Neurophysiol. 2017;128(1):243&#x02013;50. 10.1016/j.clinph.2016.11.005.<pub-id pub-id-type="pmid">27913148</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>CN</given-names></name><name><surname>Chapman</surname><given-names>KE</given-names></name><name><surname>Bear</surname><given-names>JJ</given-names></name><name><surname>Wilson</surname><given-names>SB</given-names></name><name><surname>Walleigh</surname><given-names>DJ</given-names></name><name><surname>Scheuer</surname><given-names>ML</given-names></name></person-group><article-title>Semiautomated spike detection software Persyst 13 is noninferior to human readers when calculating the spike-wave index in electrical status epilepticus in sleep</article-title><source>J Clin Neurophysiol</source><year>2018</year><volume>35</volume><issue>5</issue><fpage>370</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1097/wnp.0000000000000493</pub-id><?supplied-pmid 29933261?><pub-id pub-id-type="pmid">29933261</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Joshi CN, Chapman KE, Bear JJ, Wilson SB, Walleigh DJ, Scheuer ML. Semiautomated spike detection software Persyst 13 is noninferior to human readers when calculating the spike-wave index in electrical status epilepticus in sleep. J Clin Neurophysiol. 2018;35(5):370&#x02013;4. 10.1097/wnp.0000000000000493.<pub-id pub-id-type="pmid">29933261</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other"> Catarina Louren&#x000e7;o, Marleen C. Tjepkema-Cloostermans, Lu&#x000ed;s F. Teixeira, Putten MJAMv. Deep Learning for Interictal Epileptiform Discharge Detection from Scalp EEG Recordings. IFMBE Proc. 2019;76. 10.1007/978-3-030-31635-8_237.</mixed-citation></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>F&#x000fc;rbass</surname><given-names>F</given-names></name><name><surname>Kural</surname><given-names>MA</given-names></name><name><surname>Gritsch</surname><given-names>G</given-names></name><name><surname>Hartmann</surname><given-names>M</given-names></name><name><surname>Kluge</surname><given-names>T</given-names></name><name><surname>Beniczky</surname><given-names>S</given-names></name></person-group><article-title>An artificial intelligence-based EEG algorithm for detection of epileptiform EEG discharges: validation against the diagnostic gold standard</article-title><source>Clin Neurophysiol</source><year>2020</year><volume>131</volume><issue>6</issue><fpage>1174</fpage><lpage>1179</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2020.02.032</pub-id><?supplied-pmid 32299000?><pub-id pub-id-type="pmid">32299000</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">F&#x000fc;rbass F, Kural MA, Gritsch G, Hartmann M, Kluge T, Beniczky S. An artificial intelligence-based EEG algorithm for detection of epileptiform EEG discharges: validation against the diagnostic gold standard. Clin Neurophysiol. 2020;131(6):1174&#x02013;9. 10.1016/j.clinph.2020.02.032.<pub-id pub-id-type="pmid">32299000</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Jing</surname><given-names>J</given-names></name><name><surname>Sun</surname><given-names>H</given-names></name><name><surname>Kim</surname><given-names>JA</given-names></name><name><surname>Herlopian</surname><given-names>A</given-names></name><name><surname>Karakis</surname><given-names>I</given-names></name><name><surname>Ng</surname><given-names>M</given-names></name><etal/></person-group><article-title>Development of expert-level automated detection of epileptiform discharges during electroencephalogram interpretation</article-title><source>JAMA Neurol</source><year>2020</year><volume>77</volume><issue>1</issue><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1001/jamaneurol.2019.3485</pub-id><?supplied-pmid 31633740?><pub-id pub-id-type="pmid">31633740</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">Jing J, Sun H, Kim JA, Herlopian A, Karakis I, Ng M, et al. Development of expert-level automated detection of epileptiform discharges during electroencephalogram interpretation. JAMA Neurol. 2020;77(1):103&#x02013;8. 10.1001/jamaneurol.2019.3485.<pub-id pub-id-type="pmid">31633740</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>da Silva</surname><given-names>LC</given-names></name><name><surname>Tjepkema-Cloostermans</surname><given-names>MC</given-names></name><name><surname>van Putten</surname><given-names>M</given-names></name></person-group><article-title>Machine learning for detection of interictal epileptiform discharges</article-title><source>Clin Neurophysiol</source><year>2021</year><volume>132</volume><issue>7</issue><fpage>1433</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2021.02.403</pub-id><pub-id pub-id-type="pmid">34023625</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">da Silva LC, Tjepkema-Cloostermans MC, van Putten M. Machine learning for detection of interictal epileptiform discharges. Clin Neurophysiol. 2021;132(7):1433&#x02013;43. 10.1016/j.clinph.2021.02.403.<pub-id pub-id-type="pmid">34023625</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Nhu</surname><given-names>D</given-names></name><name><surname>Janmohamed</surname><given-names>M</given-names></name><name><surname>Shakhatreh</surname><given-names>L</given-names></name><name><surname>Gonen</surname><given-names>O</given-names></name><name><surname>Perucca</surname><given-names>P</given-names></name><name><surname>Gilligan</surname><given-names>A</given-names></name><etal/></person-group><article-title>Automated interictal epileptiform discharge detection from scalp EEG using scalable time-series classification approaches</article-title><source>Int J Neural Syst</source><year>2023</year><volume>33</volume><issue>01</issue><fpage>2350001</fpage><pub-id pub-id-type="pmid">36599664</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Nhu D, Janmohamed M, Shakhatreh L, Gonen O, Perucca P, Gilligan A, et al. Automated interictal epileptiform discharge detection from scalp EEG using scalable time-series classification approaches. Int J Neural Syst. 2023;33(01):2350001.<pub-id pub-id-type="pmid">36599664</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Fukumori</surname><given-names>K</given-names></name><name><surname>Yoshida</surname><given-names>N</given-names></name><name><surname>Sugano</surname><given-names>H</given-names></name><name><surname>Nakajima</surname><given-names>M</given-names></name><name><surname>Tanaka</surname><given-names>T</given-names></name></person-group><article-title>Epileptic spike detection using neural networks with linear-phase convolutions</article-title><source>IEEE J Biomed Health Inform</source><year>2021</year><volume>26</volume><issue>3</issue><fpage>1045</fpage><lpage>1056</lpage></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Fukumori K, Yoshida N, Sugano H, Nakajima M, Tanaka T. Epileptic spike detection using neural networks with linear-phase convolutions. IEEE J Biomed Health Inform. 2021;26(3):1045&#x02013;56.</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other"> Geng D, Alkhachroum A, Melo Bicchi MA, Jagid JR, Cajigas I, Chen ZS. Deep learning for robust detection of interictal epileptiform discharges. J Neural Eng. 2021;18(5). 10.1088/1741-2552/abf28e.</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other"> Thomas J, Comoretto L, Jin J, Dauwels J, Cash SS, Westover MB, editors. EEG classification via convolutional neural network-based interictal epileptiform event detection. 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). IEEE 3148&#x02013;3151.</mixed-citation></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Kural</surname><given-names>MA</given-names></name><name><surname>Jing</surname><given-names>J</given-names></name><name><surname>F&#x000fc;rbass</surname><given-names>F</given-names></name><name><surname>Perko</surname><given-names>H</given-names></name><name><surname>Qerama</surname><given-names>E</given-names></name><name><surname>Johnsen</surname><given-names>B</given-names></name><etal/></person-group><article-title>Accurate identification of EEG recordings with interictal epileptiform discharges using a hybrid approach: artificial intelligence supervised by human experts</article-title><source>Epilepsia</source><year>2022</year><volume>63</volume><issue>5</issue><fpage>1064</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1111/epi.17206</pub-id><?supplied-pmid 35184276?><pub-id pub-id-type="pmid">35184276</pub-id>
</element-citation><mixed-citation id="mc-CR24" publication-type="journal">Kural MA, Jing J, F&#x000fc;rbass F, Perko H, Qerama E, Johnsen B, et al. Accurate identification of EEG recordings with interictal epileptiform discharges using a hybrid approach: artificial intelligence supervised by human experts. Epilepsia. 2022;63(5):1064&#x02013;73. 10.1111/epi.17206.<pub-id pub-id-type="pmid">35184276</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>N</given-names></name><name><surname>Gao</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Liang</surname><given-names>Z</given-names></name><name><surname>Yuan</surname><given-names>G</given-names></name><etal/></person-group><article-title>vEpiNet: a multimodal interictal epileptiform discharge detection method based on video and electroencephalogram data</article-title><source>Neural Netw</source><year>2024</year><volume>175</volume><fpage>106319</fpage><pub-id pub-id-type="doi">10.1016/j.neunet.2024.106319</pub-id><?supplied-pmid 38640698?><pub-id pub-id-type="pmid">38640698</pub-id>
</element-citation><mixed-citation id="mc-CR25" publication-type="journal">Lin N, Gao W, Li L, Chen J, Liang Z, Yuan G, et al. vEpiNet: a multimodal interictal epileptiform discharge detection method based on video and electroencephalogram data. Neural Netw. 2024;175:106319. 10.1016/j.neunet.2024.106319.<pub-id pub-id-type="pmid">38640698</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Sinha</surname><given-names>SR</given-names></name><name><surname>Sullivan</surname><given-names>LR</given-names></name><name><surname>Sabau</surname><given-names>D</given-names></name><name><surname>Orta</surname><given-names>DSJ</given-names></name><name><surname>Dombrowski</surname><given-names>KE</given-names></name><name><surname>Halford</surname><given-names>JJ</given-names></name><etal/></person-group><article-title>American Clinical Neurophysiology Society guideline 1: minimum technical requirements for performing clinical electroencephalography</article-title><source>Neurodiagn J</source><year>2016</year><volume>56</volume><issue>4</issue><fpage>235</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1080/21646821.2016.1245527</pub-id><?supplied-pmid 28436800?><pub-id pub-id-type="pmid">28436800</pub-id>
</element-citation><mixed-citation id="mc-CR26" publication-type="journal">Sinha SR, Sullivan LR, Sabau D, Orta DSJ, Dombrowski KE, Halford JJ, et al. American Clinical Neurophysiology Society guideline 1: minimum technical requirements for performing clinical electroencephalography. Neurodiagn J. 2016;56(4):235&#x02013;44. 10.1080/21646821.2016.1245527.<pub-id pub-id-type="pmid">28436800</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Seeck</surname><given-names>M</given-names></name><name><surname>Koessler</surname><given-names>L</given-names></name><name><surname>Bast</surname><given-names>T</given-names></name><name><surname>Leijten</surname><given-names>F</given-names></name><name><surname>Michel</surname><given-names>C</given-names></name><name><surname>Baumgartner</surname><given-names>C</given-names></name><etal/></person-group><article-title>The standardized EEG electrode array of the IFCN</article-title><source>Clin Neurophysiol</source><year>2017</year><volume>128</volume><issue>10</issue><fpage>2070</fpage><lpage>2077</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2017.06.254</pub-id><?supplied-pmid 28778476?><pub-id pub-id-type="pmid">28778476</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Seeck M, Koessler L, Bast T, Leijten F, Michel C, Baumgartner C, et al. The standardized EEG electrode array of the IFCN. Clin Neurophysiol. 2017;128(10):2070&#x02013;7. 10.1016/j.clinph.2017.06.254.<pub-id pub-id-type="pmid">28778476</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Nolan</surname><given-names>H</given-names></name><name><surname>Whelan</surname><given-names>R</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name></person-group><article-title>FASTER: fully automated statistical thresholding for EEG artifact rejection</article-title><source>J Neurosci Methods</source><year>2010</year><volume>192</volume><issue>1</issue><fpage>152</fpage><lpage>162</lpage><?supplied-pmid 20654646?><pub-id pub-id-type="pmid">20654646</pub-id>
</element-citation><mixed-citation id="mc-CR28" publication-type="journal">Nolan H, Whelan R, Reilly RB. FASTER: fully automated statistical thresholding for EEG artifact rejection. J Neurosci Methods. 2010;192(1):152&#x02013;62.<pub-id pub-id-type="pmid">20654646</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Prasanth</surname><given-names>T</given-names></name><name><surname>Thomas</surname><given-names>J</given-names></name><name><surname>Yuvaraj</surname><given-names>R</given-names></name><name><surname>Jing</surname><given-names>J</given-names></name><name><surname>Cash</surname><given-names>SS</given-names></name><name><surname>Chaudhari</surname><given-names>R</given-names></name><etal/></person-group><article-title>Deep Learning for Interictal Epileptiform Spike Detection from scalp EEG frequency sub bands</article-title><source>Annu Int Conf IEEE Eng Med Biol Soc</source><year>2020</year><volume>2020</volume><fpage>3703</fpage><lpage>3706</lpage><pub-id pub-id-type="doi">10.1109/embc44109.2020.9175644</pub-id><?supplied-pmid 33018805?><pub-id pub-id-type="pmid">33018805</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Prasanth T, Thomas J, Yuvaraj R, Jing J, Cash SS, Chaudhari R, et al. Deep Learning for Interictal Epileptiform Spike Detection from scalp EEG frequency sub bands. Annu Int Conf IEEE Eng Med Biol Soc. 2020;2020:3703&#x02013;6. 10.1109/embc44109.2020.9175644.<pub-id pub-id-type="pmid">33018805</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>B</given-names></name><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Shi</surname><given-names>L</given-names></name><name><surname>Xu</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name></person-group><article-title>A deep learning framework with multi-perspective fusion for interictal epileptiform discharges detection in scalp electroencephalogram</article-title><source>J Neural Eng</source><year>2021</year><pub-id pub-id-type="doi">10.1088/1741-2552/ac0d60</pub-id><?supplied-pmid 34607322?><pub-id pub-id-type="pmid">34607322</pub-id>
</element-citation><mixed-citation id="mc-CR30" publication-type="journal">Wei B, Zhao X, Shi L, Xu L, Liu T, Zhang J. A deep learning framework with multi-perspective fusion for interictal epileptiform discharges detection in scalp electroencephalogram. J Neural Eng. 2021. 10.1088/1741-2552/ac0d60.<pub-id pub-id-type="pmid">34607322</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>J</given-names></name><name><surname>Thangavel</surname><given-names>P</given-names></name><name><surname>Peh</surname><given-names>WY</given-names></name><name><surname>Jing</surname><given-names>J</given-names></name><name><surname>Yuvaraj</surname><given-names>R</given-names></name><name><surname>Cash</surname><given-names>SS</given-names></name><etal/></person-group><article-title>Automated adult epilepsy diagnostic tool based on interictal scalp electroencephalogram characteristics: a six-center study</article-title><source>Int J Neural Syst</source><year>2021</year><volume>31</volume><issue>5</issue><fpage>2050074</fpage><pub-id pub-id-type="doi">10.1142/s0129065720500744</pub-id><?supplied-pmid 33438530?><pub-id pub-id-type="pmid">33438530</pub-id>
</element-citation><mixed-citation id="mc-CR31" publication-type="journal">Thomas J, Thangavel P, Peh WY, Jing J, Yuvaraj R, Cash SS, et al. Automated adult epilepsy diagnostic tool based on interictal scalp electroencephalogram characteristics: a six-center study. Int J Neural Syst. 2021;31(5):2050074. 10.1142/s0129065720500744.<pub-id pub-id-type="pmid">33438530</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>da Silva Louren&#x000e7;o</surname><given-names>C</given-names></name><name><surname>Tjepkema-Cloostermans</surname><given-names>MC</given-names></name><name><surname>van Putten</surname><given-names>M</given-names></name></person-group><article-title>Efficient use of clinical EEG data for deep learning in epilepsy</article-title><source>Clin Neurophysiol</source><year>2021</year><volume>132</volume><issue>6</issue><fpage>1234</fpage><lpage>1240</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2021.01.035</pub-id><?supplied-pmid 33867258?><pub-id pub-id-type="pmid">33867258</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">da Silva Louren&#x000e7;o C, Tjepkema-Cloostermans MC, van Putten M. Efficient use of clinical EEG data for deep learning in epilepsy. Clin Neurophysiol. 2021;132(6):1234&#x02013;40. 10.1016/j.clinph.2021.01.035.<pub-id pub-id-type="pmid">33867258</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Lv</surname><given-names>X</given-names></name><etal/></person-group><article-title>A two-stage automatic system for detection of interictal epileptiform discharges from scalp electroencephalograms</article-title><source>eNeuro</source><year>2023</year><pub-id pub-id-type="doi">10.1523/eneuro.0111-23.2023</pub-id><?supplied-pmid 37977825?><pub-id pub-id-type="pmid">37977825</pub-id>
</element-citation><mixed-citation id="mc-CR33" publication-type="journal">Wang X, Wang X, Wang C, Wang Z, Liu X, Lv X, et al. A two-stage automatic system for detection of interictal epileptiform discharges from scalp electroencephalograms. eNeuro. 2023. 10.1523/eneuro.0111-23.2023.<pub-id pub-id-type="pmid">37977825</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Chikkerur</surname><given-names>S</given-names></name><name><surname>Cartwright</surname><given-names>AN</given-names></name><name><surname>Govindaraju</surname><given-names>V</given-names></name></person-group><article-title>Fingerprint enhancement using STFT analysis</article-title><source>Pattern Recogn</source><year>2007</year><volume>40</volume><issue>1</issue><fpage>198</fpage><lpage>211</lpage></element-citation><mixed-citation id="mc-CR34" publication-type="journal">Chikkerur S, Cartwright AN, Govindaraju V. Fingerprint enhancement using STFT analysis. Pattern Recogn. 2007;40(1):198&#x02013;211.</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><citation-alternatives><element-citation id="ec-CR35" publication-type="journal"><person-group person-group-type="author"><name><surname>Quon</surname><given-names>RJ</given-names></name><name><surname>Meisenhelter</surname><given-names>S</given-names></name><name><surname>Camp</surname><given-names>EJ</given-names></name><name><surname>Testorf</surname><given-names>ME</given-names></name><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Song</surname><given-names>Q</given-names></name><etal/></person-group><article-title>AiED: artificial intelligence for the detection of intracranial interictal epileptiform discharges</article-title><source>Clin Neurophysiol</source><year>2022</year><volume>133</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2021.09.018</pub-id><?supplied-pmid 34773796?><pub-id pub-id-type="pmid">34773796</pub-id>
</element-citation><mixed-citation id="mc-CR35" publication-type="journal">Quon RJ, Meisenhelter S, Camp EJ, Testorf ME, Song Y, Song Q, et al. AiED: artificial intelligence for the detection of intracranial interictal epileptiform discharges. Clin Neurophysiol. 2022;133:1&#x02013;8. 10.1016/j.clinph.2021.09.018.<pub-id pub-id-type="pmid">34773796</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other"> Tan M, Le Q, editors. Efficientnetv2: Smaller models and faster training. International conference on machine learning; 2021: 10096&#x02013;10106 PMLR.</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other"> Redmon J, Divvala S, Girshick R, Farhadi A, editors. You only look once: Unified, real-time object detection. Proceedings of the IEEE conference on computer vision and pattern recognition; 2016:779&#x02013;788.</mixed-citation></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>P</given-names></name><name><surname>Ergu</surname><given-names>D</given-names></name><name><surname>Liu</surname><given-names>F</given-names></name><name><surname>Cai</surname><given-names>Y</given-names></name><name><surname>Ma</surname><given-names>B</given-names></name></person-group><article-title>A review of Yolo algorithm developments</article-title><source>Procedia Computer Science</source><year>2022</year><volume>199</volume><fpage>1066</fpage><lpage>1073</lpage></element-citation><mixed-citation id="mc-CR38" publication-type="journal">Jiang P, Ergu D, Liu F, Cai Y, Ma B. A review of Yolo algorithm developments. Procedia Computer Science. 2022;199:1066&#x02013;73.</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Singla</surname><given-names>N</given-names></name></person-group><article-title>Motion detection based on frame difference method</article-title><source>International Journal of Information &#x00026; Computation Technology</source><year>2014</year><volume>4</volume><issue>15</issue><fpage>1559</fpage><lpage>1565</lpage></element-citation><mixed-citation id="mc-CR39" publication-type="journal">Singla N. Motion detection based on frame difference method. International Journal of Information &#x00026; Computation Technology. 2014;4(15):1559&#x02013;65.</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Hintze</surname><given-names>JL</given-names></name><name><surname>Nelson</surname><given-names>RD</given-names></name></person-group><article-title>Violin plots: a box plot-density trace synergism</article-title><source>Am Stat</source><year>1998</year><volume>52</volume><issue>2</issue><fpage>181</fpage><lpage>184</lpage></element-citation><mixed-citation id="mc-CR40" publication-type="journal">Hintze JL, Nelson RD. Violin plots: a box plot-density trace synergism. Am Stat. 1998;52(2):181&#x02013;4.</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Y</given-names></name></person-group><article-title>An evaluation of statistical approaches to text categorization</article-title><source>Inf Retrieval</source><year>1999</year><volume>1</volume><issue>1</issue><fpage>69</fpage><lpage>90</lpage></element-citation><mixed-citation id="mc-CR41" publication-type="journal">Yang Y. An evaluation of statistical approaches to text categorization. Inf Retrieval. 1999;1(1):69&#x02013;90.</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other"> Nhu D, Janmohamed M, Antonic-Baker A, Perucca P, O'Brien TJ, Gilligan AK, et al. Deep learning for automated epileptiform discharge detection from scalp EEG: A systematic review. J Neural Eng. 2022;19(5). 10.1088/1741-2552/ac9644.</mixed-citation></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Borges Camargo Diniz</surname><given-names>J</given-names></name><name><surname>Silva Santana</surname><given-names>L</given-names></name><name><surname>Leite</surname><given-names>M</given-names></name><name><surname>Silva Santana</surname><given-names>JL</given-names></name><name><surname>Magalh&#x000e3;es Costa</surname><given-names>SI</given-names></name><name><surname>Martins Castro</surname><given-names>LH</given-names></name><etal/></person-group><article-title>Advancing epilepsy diagnosis: a meta-analysis of artificial intelligence approaches for interictal epileptiform discharge detection</article-title><source>Seizure</source><year>2024</year><volume>122</volume><fpage>80</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/j.seizure.2024.09.019</pub-id><?supplied-pmid 39369555?><pub-id pub-id-type="pmid">39369555</pub-id>
</element-citation><mixed-citation id="mc-CR43" publication-type="journal">Borges Camargo Diniz J, Silva Santana L, Leite M, Silva Santana JL, Magalh&#x000e3;es Costa SI, Martins Castro LH, et al. Advancing epilepsy diagnosis: a meta-analysis of artificial intelligence approaches for interictal epileptiform discharge detection. Seizure. 2024;122:80&#x02013;6. 10.1016/j.seizure.2024.09.019.<pub-id pub-id-type="pmid">39369555</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name><surname>Tjepkema-Cloostermans</surname><given-names>MC</given-names></name><name><surname>de Carvalho</surname><given-names>RC</given-names></name><name><surname>van Putten</surname><given-names>MJ</given-names></name></person-group><article-title>Deep learning for detection of focal epileptiform discharges from scalp EEG recordings</article-title><source>Clin Neurophysiol</source><year>2018</year><volume>129</volume><issue>10</issue><fpage>2191</fpage><lpage>2196</lpage><?supplied-pmid 30025804?><pub-id pub-id-type="pmid">30025804</pub-id>
</element-citation><mixed-citation id="mc-CR44" publication-type="journal">Tjepkema-Cloostermans MC, de Carvalho RC, van Putten MJ. Deep learning for detection of focal epileptiform discharges from scalp EEG recordings. Clin Neurophysiol. 2018;129(10):2191&#x02013;6.<pub-id pub-id-type="pmid">30025804</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>da Silva</surname><given-names>LC</given-names></name><name><surname>Tjepkema-Cloostermans</surname><given-names>MC</given-names></name><name><surname>van Putten</surname><given-names>MJ</given-names></name></person-group><article-title>Not one size fits all: influence of EEG type when training a deep neural network for interictal epileptiform discharge detection</article-title><source>Informatics in Medicine Unlocked</source><year>2023</year><volume>41</volume><fpage>101318</fpage></element-citation><mixed-citation id="mc-CR45" publication-type="journal">da Silva LC, Tjepkema-Cloostermans MC, van Putten MJ. Not one size fits all: influence of EEG type when training a deep neural network for interictal epileptiform discharge detection. Informatics in Medicine Unlocked. 2023;41:101318.</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Cao</surname><given-names>J</given-names></name><name><surname>Bao</surname><given-names>Z</given-names></name><name><surname>Jiang</surname><given-names>T</given-names></name><name><surname>Gao</surname><given-names>F</given-names></name></person-group><article-title>BECT spike detection based on novel EEG sequence features and LSTM algorithms</article-title><source>IEEE Trans Neural Syst Rehabil Eng</source><year>2021</year><volume>29</volume><fpage>1734</fpage><lpage>1743</lpage><?supplied-pmid 34428145?><pub-id pub-id-type="pmid">34428145</pub-id>
</element-citation><mixed-citation id="mc-CR46" publication-type="journal">Xu Z, Wang T, Cao J, Bao Z, Jiang T, Gao F. BECT spike detection based on novel EEG sequence features and LSTM algorithms. IEEE Trans Neural Syst Rehabil Eng. 2021;29:1734&#x02013;43.<pub-id pub-id-type="pmid">34428145</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Jiang</surname><given-names>J</given-names></name><name><surname>Xiao</surname><given-names>N</given-names></name><name><surname>Guo</surname><given-names>J</given-names></name><name><surname>Zhuang</surname><given-names>K</given-names></name><etal/></person-group><article-title>Automatic interictal epileptiform discharge (IED) detection based on convolutional neural network (CNN)</article-title><source>Front Mol Biosci</source><year>2023</year><volume>10</volume><fpage>1146606</fpage><pub-id pub-id-type="doi">10.3389/fmolb.2023.1146606</pub-id><?supplied-pmid 37091867?><pub-id pub-id-type="pmid">37091867</pub-id>
</element-citation><mixed-citation id="mc-CR47" publication-type="journal">Zhang L, Wang X, Jiang J, Xiao N, Guo J, Zhuang K, et al. Automatic interictal epileptiform discharge (IED) detection based on convolutional neural network (CNN). Front Mol Biosci. 2023;10:1146606. 10.3389/fmolb.2023.1146606.<pub-id pub-id-type="pmid">37091867</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>